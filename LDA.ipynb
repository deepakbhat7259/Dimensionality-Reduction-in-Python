{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgcUWyMHoMtSpmKkKlGFX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanpaul21/Dimensionality-Reduction-in-Python/blob/master/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbiUXFYJG-YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA\n",
        "\n",
        "# PCA is an unsupervised while LDA is a supervised dimensionality reduction technique.\n",
        "# PCA has no concern with the class labels. \n",
        "# In simple words, PCA summarizes the feature set without relying on the output. \n",
        "# PCA tries to find the directions of the maximum variance in the dataset. \n",
        "# In a large feature set, there are many features that are merely duplicate of the other features \n",
        "# or have a high correlation with the other features. \n",
        "# Such features are basically redundant and can be ignored. \n",
        "# The role of PCA is to find such highly correlated or duplicate features \n",
        "# and to come up with a new feature set where there is minimum correlation between the features \n",
        "# In other words feature set with maximum variance between the features. \n",
        "# Since the variance between the features doesn't depend upon the output, \n",
        "# therefore PCA doesn't take the output labels into account.\n",
        "\n",
        "# Unlike PCA, LDA tries to reduce dimensions of the feature set while retaining the information \n",
        "# that discriminates output classes. \n",
        "# LDA tries to find a decision boundary around each cluster of a class. \n",
        "# It then projects the data points to new dimensions in a way that the clusters are as separate \n",
        "# from each other as possible and the individual elements within a cluster \n",
        "# are as close to the centroid of the cluster as possible. \n",
        "# The new dimensions are ranked on the basis of their ability to maximize the distance \n",
        "# between the clusters and minimize the distance between the data points within a cluster and\n",
        "# their centroids. These new dimensions form the linear discriminants of the feature set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HITpc9EHd-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "8f38f5a4-c9bb-4771-825f-6e1f482f6675"
      },
      "source": [
        "# Libraries \n",
        "from sklearn import datasets \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hSgw_mzHuwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "45640e17-d254-45ae-b365-e2e95f2baf69"
      },
      "source": [
        "# Load the Datasets \n",
        "\n",
        "# Iris Dataset for Classification\n",
        "# Load Dataset\n",
        "iris = datasets.load_iris()\n",
        "# Convert to DataFrame\n",
        "iris_pd = pd.DataFrame(iris.data)\n",
        "# Feature Name\n",
        "iris_pd.columns = iris.feature_names\n",
        "# Target Variable\n",
        "iris_pd[\"Class\"] = iris.target\n",
        "dataset = iris_pd\n",
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  Class\n",
              "0                5.1               3.5  ...               0.2      0\n",
              "1                4.9               3.0  ...               0.2      0\n",
              "2                4.7               3.2  ...               0.2      0\n",
              "3                4.6               3.1  ...               0.2      0\n",
              "4                5.0               3.6  ...               0.2      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mXk33AWH6eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# Once dataset is loaded into a pandas data frame object, \n",
        "# the first step is to divide dataset into features and corresponding labels \n",
        "# and then divide the resultant dataset into training and test sets. \n",
        "\n",
        "X = dataset.iloc[:, 0:4].values\n",
        "y = dataset.iloc[:, 4].values\n",
        "\n",
        "# Inference : \n",
        "# The above script assigns the first four columns of the dataset i.e. the feature set to X variable \n",
        "# while the values in the fifth column (labels) are assigned to the y variable."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7YnrzH_IRQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training and Test sets (80:20)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZquO79KIezZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Scaling\n",
        "\n",
        "# As was the case with PCA, we need to perform feature scaling for LDA too. \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvGcJiUhIlt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA(n_components=1)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Inference : \n",
        "#  Like PCA, we have to pass the value for the n_components parameter of the LDA, \n",
        "# which refers to the number of linear discriminates that we want to retrieve. \n",
        "# In this case we set the n_components to 1, since we first want to check the performance \n",
        "# of our classifier with a single linear discriminant. \n",
        "# Finally we execute the fit and transform methods to actually retrieve the linear discriminants.\n",
        "# Notice, in case of LDA, the transform method takes two parameters: the X_train and the y_train. \n",
        "# However in the case of PCA, the transform method only requires one parameter i.e. X_train. \n",
        "# This reflects the fact that LDA takes the output class labels into account \n",
        "# while selecting the linear discriminants, while PCA doesn't depend upon the output labels."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3ngyXIEI94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training and Making Predictions\n",
        "\n",
        "# Since we want to compare the performance of LDA with one linear discriminant \n",
        "# to the performance of PCA with one principal component, \n",
        "# we will use the same Random Forest classifier that we used to \n",
        "# evaluate performance of PCA-reduced algorithms.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoIPLIlVJW6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "3a9bc46b-28f4-4f0e-988f-5df580e53f67"
      },
      "source": [
        "# Evaluating the Performance\n",
        "\n",
        "# As always, the last step is to evaluate performance of the algorithm \n",
        "# with the help of a confusion matrix and find the accuracy of the prediction. \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print('Accuracy' + str(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "# Inference : \n",
        "# You can see that with one linear discriminant, the algorithm achieved an accuracy of 100%"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0  6]]\n",
            "Accuracy1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHHT96lYJhb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA vs LDA: What to Choose for Dimensionality Reduction?\n",
        "# In case of uniformly distributed data, LDA almost always performs better than PCA.\n",
        "# However if the data is highly skewed (irregularly distributed) \n",
        "# then it is advised to use PCA since LDA can be biased towards the majority class.\n",
        "\n",
        "# Finally, it is beneficial that PCA can be applied to labeled as well as unlabeled data \n",
        "# since it doesn't rely on the output labels. \n",
        "# On the other hand, LDA requires output classes for finding linear discriminants and \n",
        "# hence requires labeled data."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}